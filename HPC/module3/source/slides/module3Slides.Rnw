        
\documentclass[table]{beamer}


\input{../statsTeachR_preamble_slides}

%  The following variables are assumed by the standard preamble:
%	Global variable containing module name:
\title{Cluster Computing}

%        Global variable containing text of authorship acknowledgments and license terms:
\newcommand{\LicenseText}{These slides were adapted for \href{http://statsteachr.org}{statsTeachR} by Emily Ramos from slides written by Andrea S Foulkes, Gregory J Matthews, Nicholas G Reich and are released under a \href{http://creativecommons.org/licenses/by-sa/3.0/deed.en_US}{Creative Commons Attribution-ShareAlike 3.0 Unported License}. }



%	******	Document body begins here	**********************

\begin{document}

%  Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 
\begin{frame}{Overview}
\begin{itemize}
\item Parallel computing allows us (for certain problems) to split a big job up into many smaller parts and run them in parallel.  
\item Our laptops or desktops can split a job up into as many threads as are available.  
\item This is likely a relatively small number like 4, 8, or 12. 
 \end{itemize}
\end{frame}

\begin{frame}{Overview}
\begin{itemize}
\item {\bf Cluster computing} connects many computers (nodes) together on a local network. 
\item This allows a pooling of resources to increase computing power.
\item This allows a user to access hundreds or even {\it thousands} of cores (if they need them). 
\end{itemize}
\end{frame}

\begin{frame}{MGHPCC}
\begin{itemize}
\item The Massachusetts Green High Performance Computing Center (MGHPCC) is one of these clusters. 
\item Each of the participating instiutions has their own distinct cluster. We will be using the UMass cluster.
\item In order to connect to this cluster, a user has to be on the UMass campus (or use VPN to the UMass campus.)
\item We will need to VPN into the UMass network since we are currently off campus.  
\end{itemize}
\end{frame}


\begin{frame}{Hypothetical cluster computing workflow}

\begin{itemize}
        \item Transfer data/scripts to cluster
        \item Log in to the cluster
        \item Submit a job to the ``scheduler''
        \item Transfer data/results from cluster if needed
\end{itemize}

\end{frame}


\begin{frame}{The process we will use today}

\begin{block}{Our order of operations...}
\begin{itemize}
        \item Log in to the cluster
        \item Transfer data/scripts to cluster
        \item Submit a job to the ``scheduler''
        \item Look at results
\end{itemize}
\end{block}

\end{frame}


\begin{frame}{The process we will use today}

\begin{block}{Our order of operations...}
\begin{itemize}
        \item {\bf Log in to the cluster}
        \item Transfer data/scripts to cluster
        \item Submit a job to the ``scheduler''
        \item Look at results
\end{itemize}
\end{block}

\end{frame}



\begin{frame}{How to pretend you are at UMass using VPN}
\begin{itemize}
\item Mac: System Preferences $>$ Network $>$ VPN on left 
\item Windows: Cisco VPN client
\begin{itemize}
\item Server address: vpn2.oit.umass.edu 
\item Group name: umass
\item Password: vpn4umass
\item Account name: UMass user ID OR bip[number] (temporary for BIP2014)
\item Password: Enter your password
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Accessing the MGHPCC}
\begin{itemize}
\item Register for the MGHPCC (for UMass): https://www.umassrc.org/hpc/
\item A login and password will be (have been?) assigned to you. 
\item At the moment, this will be different than your UMass user ID or email address. (NetID integration coming soon!)
\end{itemize}
\end{frame}


\begin{frame}{Accessing the MGHPCC}
\begin{itemize}
\item Unix or Mac
\begin{itemize}
\item Unix or Mac: Create a secure shell: ssh username@ghpcc06.umassrc.org
\end{itemize}
\item Windows
\begin{itemize}
\item PuTTY: ghpcc06.umassrc.org (you will be prompted for user name and password)
\end{itemize}
\item Enter password when prompted. (First time you login, you will need to change your password, then login again.)  
\item You are now connected to the cluster!
\item Remember: You can only access the MGHPCC if you are on UMass's network (physically on campus or VPN)
\end{itemize}
\end{frame}



\begin{frame}{The process we will use today}

\begin{block}{Our order of operations...}
\begin{itemize}
        \item Log in to the cluster
        \item {\bf Transfer data/scripts to cluster}
        \item Submit a job to the ``scheduler''
        \item Look at results
\end{itemize}
\end{block}

\end{frame}



\begin{frame}{Transferring Data to MGHPCC}
\begin{itemize}
\item We recommend using Cyberduck, a graphical SFTP client. 
\item Other options include scp and sftp for Mac/Unix command line users or PSCP/PSFTP for Windows users.  
\item Let's try to move the BiPSandbox folder to your /home/username folder on the MGHPCC.  
\end{itemize}
\end{frame}


\begin{frame}{The process we will use today}

\begin{block}{Our order of operations...}
\begin{itemize}
        \item Log in to the cluster
        \item Transfer data/scripts to cluster
        \item {\bf Submit a job to the ``scheduler''}
        \item Look at results
\end{itemize}
\end{block}

\end{frame}



\begin{frame}{Software available on the MGHPCC}
\begin{itemize}
\item Once logged in, you will be placed in your home directory (/home/username). 
\item Full list of available software: http://wiki.umassrc.org/wiki/index.php/Provided\_Software  
\item There is a wide array of available software options. 
\end{itemize}
\end{frame}

\begin{frame}{Using software on the MGHPCC}
\begin{itemize}
\item In order to use software on the server, you'll need to load modules that contain the software.  
\item We are interested in using R here.  
\item To load R we use the command: {\tt module load R/3.0.1}
\item We can also unload R with the command:  {\tt module unload R}
\end{itemize}
\end{frame}

\begin{frame}{Software}
\begin{itemize}
\item Loading the R module will allow us to run R interactively (by typing R at the prompt) on the MGHPCC servers.  
\item But you should not do this!
\end{itemize}
\end{frame}

\begin{frame}{Batch Mode}
\begin{itemize}
\item Rather than running R code interactively, we can also run R in BATCH mode. 
\item This first requires you to write an R script that you want to run.  
\item Then at the command prompt, you can submit the script by typing: {\tt R CMD BATCH [options] scriptName.R}
\item But you should not do this either!
\item We want to submit batch jobs to the cluster, not to the machine that you login to.  
\end{itemize}
\end{frame}

\begin{frame}{Submitting jobs to the cluster}
\begin{itemize}
\item In order to submit a job to the cluster we need: 
\begin{itemize}
\item A R script that we wish to run.
\item A shell script that calls the R script and submits the job to the cluster.  
\end{itemize}
\item We will then submit the job to the LSF (Load Sharing Facility) scheduler.  
\item {\bf Note}: Commands submitted to LSF are just like if they were run from a command prompt. 
\end{itemize}
\end{frame}


\begin{frame}{LSF common commands}
\begin{itemize}
\item {\tt bsub} - submit a job
\item {\tt bkill} - kill a job
\item {\tt bjobs} - view status of jobs
\item {\tt bpeek} - view output / error files
\item {\tt bhist} - job history
\item {\tt bqueues} - available queues
\end{itemize}
\end{frame}







\begin{frame}{Batch Mode}
\begin{itemize}
\item Once we have a .R file that we want to run we can submit it to the LSF job scheduler.  
\item What actually gets submited to the LSF scheduler is a shell script that calls the R file (which we'll call example.R). 
\item We need to create a file that contains all of the code we would have run at the shell prompt.  
\item Example of a shell script is below. We'll call this shell script example.sh
\end{itemize}
{\tt \$ module load R/3.0.1 }\\
{\tt \$ R CMD BATCH --vanilla example.R}\\
\end{frame}

\begin{frame}{Submiting jobs to the cluster}
\begin{itemize}
\item The command {\bf bsub} allows us to submit the shell script to the cluster. 
\item The command below will submit the shell script example.sh to the cluster.  
\end{itemize}
{\tt \$ bsub example.sh}
\end{frame}


\begin{frame}{Submiting jobs to the cluster: Options}
\begin{itemize}
\item We can also set many options in our job submission. 
\item -n: Number of cores requested
\item -W: Wall clock time
\item -R: Memory per job
\item -q: Which queue to submit to
\end{itemize}
{\tt \$ bsub -n 4 -R "rusage[mem=2048]" -W 0:10 -q long example.sh}
\end{frame}

\begin{frame}{Submiting jobs to the cluster: Local}
\begin{itemize}
\item An example shell script is in \\ {\tt module3/labs/submitLocalParallelJob.sh}. 
\item Here we submit to the short queue.
\item Commands to submit from within a .sh file is the same as when submitting from the command line: \\ {\tt bsub $<$ submitLocalParallelJob.sh}
\end{itemize}

\end{frame}

\begin{frame}{Submiting jobs to the cluster: Distributed}
\begin{itemize}
\item We can also specify options in the shell script that we submit to the LSF scheduler.  
\item An example shell script is in \\
{\tt module3/labs/submitDistrParallelJob.sh}. 
\item Commands to submit is again: \\ {\tt bsub $<$ submitDistrParallelJob.sh}
\item ``Proper'' way to distribute your job is using something like MPI.
\item In this example, output is also distributed, less convenient.
\end{itemize}

\end{frame}



\end{document}
